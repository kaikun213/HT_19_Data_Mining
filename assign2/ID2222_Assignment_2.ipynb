{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYprUY0irzMN"
   },
   "source": [
    "## **Assingment 2** - Group 50\n",
    "\n",
    "LÃ¼tfi Altin (lutfia@kth.se) |\n",
    "Jakob Heyder (heyder@kth.se)\n",
    "\n",
    "### Task:\n",
    "You are to solve the first sub-problem: to implement the Apriori algorithm for finding frequent itemsets with support at least **_s_** in a dataset of sales transactions. Remind that support of an itemset is the number of transactions containing the itemset. To test and evaluate your implementation, write a program that uses your Apriori algorithm implementation to discover frequent itemsets with support at least s in a given dataset of sales transactions.\n",
    "\n",
    "Solve the second sub-problem, i.e., develop and implement an algorithm for generating association rules between frequent itemsets discovered by using the Apriori algorithm in a dataset of sales transactions. The rules must have support at least **_s_** and confidence at least **_c_**, where **_s_** and **_c_** are given as input parameters.\n",
    "\n",
    "\n",
    "### Dataset and Tools\n",
    "\n",
    "The classes will be implemented as Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "UvFiCSJTrunx"
   },
   "outputs": [],
   "source": [
    "# Load dependencies (pandas, csv etc.)\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "b9sFotpOjxdn",
    "outputId": "4b922dec-9fe0-41af-9569-e518fa8ff603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 baskets:\n",
      "[[25, 52, 164, 240, 274, 328, 368, 448, 538, 561, 630, 687, 730, 775, 825, 834],\n",
      " [39, 120, 124, 205, 401, 581, 704, 814, 825, 834],\n",
      " [35, 249, 674, 712, 733, 759, 854, 950]]\n",
      "\n",
      "Number of baskets:  100000\n"
     ]
    }
   ],
   "source": [
    "# Import sales data\n",
    "baskets = []\n",
    "\n",
    "for l in open('T10I4D100K.dat', 'r'):\n",
    "    items = l.strip().split(' ')\n",
    "    items = list(map(int, items)) # convert items to integers\n",
    "    baskets.append(items)\n",
    "\n",
    "print(\"First 3 baskets:\")\n",
    "pprint(baskets[0:3])\n",
    "\n",
    "print()\n",
    "print(\"Number of baskets: \", len(baskets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6mM5ELkzytE"
   },
   "source": [
    "Construct function: Creates possible set of frequent itemsets.\n",
    "\n",
    "**_*1_** Consider the case where there are 3 frequent items: a, b, c. Possible pairs of frequent items are (a, b), (a, c) and (b, c). Construction of possible triplets is done by merging pairs with frequent itemset, looping through frequent items and adding them to each pair. This will result in following duplicate triplets:\n",
    "* (a, b, c)\n",
    "* (a, c, b)\n",
    "* (b, c, a)\n",
    "\n",
    "To overcome the duplicates we keep an identity of each triplet and filter them if they are already in possible set of frequent items.\n",
    "\n",
    "**_*2_** Assume with 3 frequent items a, b, c following frequent pairs are found: (a, b) and (a, c). In that case (a, b, c) is not a canditate for frequent pair because (b, c) is not a frequent itemset. However this is not so important because k=2 requires most computational power for typical baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "id": "CuHfOq7MlbdK",
    "outputId": "8c7f513e-9e03-4d17-f268-4defcf8f4fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 52, 240, 274, 368, 448, 538, 561, 775, 825, 39, 120, 205, 401, 581, 704, 814, 35, 674, 733, 854, 950, 449, 895, 937, 964, 229, 283, 294, 381, 738, 766, 853, 883, 966, 978, 143, 569, 620, 798, 214, 350, 529, 658, 682, 782, 809, 947, 970, 227, 390, 71, 192, 279, 280, 496, 530, 597, 675, 720, 914, 932, 183, 193, 217, 256, 276, 653, 706, 878, 161, 175, 177, 424, 571, 623, 795, 910, 960, 125, 130, 392, 461, 801, 862, 27, 78, 921, 147, 411, 572, 579, 778, 803, 903, 266, 523, 614, 888, 944, 43, 70, 204, 334, 480, 874, 151, 830, 890, 73, 118, 310, 419, 484, 722, 810, 844, 846, 918, 967, 326, 403, 526, 774, 788, 789, 975, 116, 198, 201, 395, 171, 541, 701, 805, 946, 471, 487, 631, 638, 735, 780, 935, 17, 242, 758, 763, 956, 145, 385, 676, 790, 792, 885, 522, 617, 12, 296, 354, 548, 684, 740, 841, 210, 346, 477, 605, 829, 884, 355, 460, 746, 600, 28, 742, 5, 115, 517, 736, 744, 919, 196, 489, 494, 673, 362, 591, 31, 58, 181, 472, 573, 628, 651, 154, 168, 580, 832, 871, 988, 72, 981, 10, 132, 172, 464, 21, 32, 54, 136, 239, 348, 100, 500, 48, 126, 319, 639, 765, 521, 112, 140, 285, 387, 594, 93, 583, 606, 236, 952, 90, 593, 122, 1, 423, 516, 6, 69, 797, 913, 577, 110, 509, 995, 343, 527, 33, 158, 989, 97, 793, 598, 427, 470, 37, 55, 897, 275, 45, 162, 378, 534, 906, 912, 117, 373, 546, 665, 963, 349, 8, 413, 94, 982, 984, 515, 692, 694, 567, 57, 800, 812, 923, 160, 752, 998, 899, 710, 867, 170, 438, 357, 75, 108, 486, 440, 38, 252, 265, 686, 819, 886, 843, 129, 510, 68, 860, 887, 804, 826, 394, 707, 838, 948, 308, 661, 634, 215, 351, 405, 949, 893, 922, 173, 258, 450, 428, 769]\n",
      "filtering 53628 candidate sets with k=2\n",
      "[[368, 829], [561, 888], [346, 561], [39, 825], [217, 283], [283, 346], [33, 283], [283, 515], [529, 782], [529, 598], [217, 346], [33, 217], [217, 515], [392, 862], [801, 862], [788, 956], [789, 829], [487, 510], [58, 354], [354, 752], [438, 684], [75, 684], [33, 346], [346, 515], [33, 515], [75, 438]]\n",
      "filtering 8445 candidate sets with k=3\n",
      "[[217, 283, 346], [33, 217, 283], [217, 283, 515], [33, 283, 346], [283, 346, 515], [33, 283, 515], [33, 217, 346], [217, 346, 515], [33, 217, 515], [33, 346, 515]]\n",
      "filtering 3235 candidate sets with k=4\n",
      "[[33, 217, 283, 346], [217, 283, 346, 515], [33, 217, 283, 515], [33, 283, 346, 515], [33, 217, 346, 515]]\n",
      "filtering 1616 candidate sets with k=5\n",
      "[[33, 217, 283, 346, 515]]\n",
      "filtering 323 candidate sets with k=6\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Apriori algorithm implementation\n",
    "\n",
    "# constructs possible set of frequent itemsets with k+1 items where `freqItemsets` is itemsets with k items \n",
    "def construct(freqItemsets, freqItems):\n",
    "    itemsets = []\n",
    "    signatures = set()\n",
    "\n",
    "    for s in freqItemsets:\n",
    "        for i in freqItems:\n",
    "            if type(s) == list:\n",
    "                possibleSet = list(s)\n",
    "            else:\n",
    "                possibleSet = [s]\n",
    "\n",
    "            if i not in possibleSet: # add to possible set if not already in there\n",
    "                possibleSet.append(i)\n",
    "                possibleSet.sort()\n",
    "                signature = \",\".join(map(str, possibleSet))\n",
    "\n",
    "                if signature not in signatures: # make sure new possible set is not already generated. see notice *1\n",
    "                    signatures.add(signature)\n",
    "                    itemsets.append(possibleSet)\n",
    "\n",
    "                    # possible performance increase: check that all subsets of possibleSet are in freqItemsets. see *2\n",
    "\n",
    "    return itemsets\n",
    "    \n",
    "def filter(baskets, possibleSets, threshold):\n",
    "    print('filtering %s candidate sets with k=%s' % (len(possibleSets), len(possibleSets[0])))\n",
    "\n",
    "    # count occurences of itemsets in baskets\n",
    "    occurences = [0 for s in possibleSets] # initialize list with 0 occurences\n",
    "    for b in baskets:\n",
    "        for index, s in enumerate(possibleSets):\n",
    "\n",
    "            in_basket = True\n",
    "            for item in s:\n",
    "                if item not in b:\n",
    "                    in_basket = False\n",
    "                    break\n",
    "            \n",
    "            if in_basket:\n",
    "                occurences[index] += 1\n",
    "    \n",
    "    # filter and return frequent itemsets with occurence counts\n",
    "    return { \",\".join(map(str, possibleSets[i])):v for i, v in enumerate(occurences) if v >= threshold }\n",
    "\n",
    "def apriori(baskets, threshold):\n",
    "    occurenceData = []\n",
    "\n",
    "    # 1. count occurences of each item in baskets\n",
    "    occurences = {} # a dictionary containing occurence of items in baskets\n",
    "    for b in baskets:\n",
    "        for item in b:\n",
    "            if item not in occurences: # this item didn't exist in previous baskets, initialize with 0\n",
    "                occurences[item] = 0\n",
    "            occurences[item] += 1\n",
    "    \n",
    "    # 2. filter frequent items, itemsets can only be made of frequent items\n",
    "    occurences = {k:v for k, v in occurences.items() if v >= threshold}\n",
    "    freqItems = [k for k, v in occurences.items()]\n",
    "\n",
    "    occurenceData.append(occurences)\n",
    "    \n",
    "    # 3. continue with apriori algorithm pipeline, construct candidate k-tuples & filter\n",
    "    freqItemsets = freqItems\n",
    "    while len(freqItemsets) > 0:\n",
    "        print(freqItemsets)\n",
    "\n",
    "        occurences = filter(baskets, construct(freqItemsets, freqItems), threshold)\n",
    "        freqItemsets = [list(map(int, k.split(\",\"))) for k, v in occurences.items()]\n",
    "\n",
    "        occurenceData.append(occurences)\n",
    "\n",
    "    return occurenceData\n",
    "\n",
    "#occurenceData = apriori(baskets, 1000) # 1%\n",
    "\n",
    "occurenceData = apriori(baskets[0:1000], 12) # test with this during development, tests against a subset of data\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mqeP3CwdWxKc",
    "outputId": "f5746a7c-9ff1-4dff-da4f-66341a51bed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[801]  ==>  862\n",
      "[515]  ==>  217\n",
      "[217, 283]  ==>  346\n",
      "[217, 283]  ==>  33\n",
      "[217, 283]  ==>  515\n",
      "[283, 346]  ==>  217\n",
      "[283, 346]  ==>  33\n",
      "[283, 346]  ==>  515\n",
      "[33, 283]  ==>  217\n",
      "[33, 283]  ==>  346\n",
      "[33, 283]  ==>  515\n",
      "[283, 515]  ==>  217\n",
      "[283, 515]  ==>  346\n",
      "[283, 515]  ==>  33\n",
      "[33, 217]  ==>  283\n",
      "[33, 217]  ==>  346\n",
      "[33, 217]  ==>  515\n",
      "[217, 515]  ==>  283\n",
      "[217, 515]  ==>  346\n",
      "[217, 515]  ==>  33\n",
      "[33, 346]  ==>  283\n",
      "[33, 346]  ==>  217\n",
      "[33, 346]  ==>  515\n",
      "[346, 515]  ==>  283\n",
      "[346, 515]  ==>  217\n",
      "[346, 515]  ==>  33\n",
      "[33, 515]  ==>  283\n",
      "[33, 515]  ==>  217\n",
      "[33, 515]  ==>  346\n",
      "[217, 283, 346]  ==>  33\n",
      "[217, 283, 346]  ==>  515\n",
      "[33, 217, 283]  ==>  346\n",
      "[33, 217, 283]  ==>  515\n",
      "[217, 283, 515]  ==>  346\n",
      "[217, 283, 515]  ==>  33\n",
      "[33, 283, 346]  ==>  217\n",
      "[33, 283, 346]  ==>  515\n",
      "[283, 346, 515]  ==>  217\n",
      "[283, 346, 515]  ==>  33\n",
      "[33, 283, 515]  ==>  217\n",
      "[33, 283, 515]  ==>  346\n",
      "[33, 217, 346]  ==>  283\n",
      "[33, 217, 346]  ==>  515\n",
      "[217, 346, 515]  ==>  283\n",
      "[217, 346, 515]  ==>  33\n",
      "[33, 217, 515]  ==>  283\n",
      "[33, 217, 515]  ==>  346\n",
      "[33, 346, 515]  ==>  283\n",
      "[33, 346, 515]  ==>  217\n",
      "[33, 217, 283, 346]  ==>  515\n",
      "[217, 283, 346, 515]  ==>  33\n",
      "[33, 217, 283, 515]  ==>  346\n",
      "[33, 283, 346, 515]  ==>  217\n",
      "[33, 217, 346, 515]  ==>  283\n"
     ]
    }
   ],
   "source": [
    "def associationRules(occurenceData, confidence=0.8):\n",
    "    freqItems = occurenceData[0]\n",
    "\n",
    "    for k in range(len(occurenceData) - 1): # occurenceData contains support values for each k-itemsets\n",
    "        for itemset in occurenceData[k]:\n",
    "            supportItemset = occurenceData[k][itemset] # extract support value for current itemset, will be divided to calculate confidence\n",
    "\n",
    "            if type(itemset) == int:\n",
    "                itemset = [itemset]\n",
    "            else:\n",
    "                itemset = list(map(int, itemset.split(\",\")))\n",
    "\n",
    "            for newItem in freqItems: # loop through `freqItems` and add each one of them to current itemset. same as construct\n",
    "                newItemset = list(itemset)\n",
    "                newItemset.append(newItem)\n",
    "                newItemset.sort()\n",
    "                newItemset = \",\".join(map(str, newItemset)) # generate new itemset with `newItem` added to itemset\n",
    "\n",
    "                if newItemset not in occurenceData[k+1]:\n",
    "                    continue\n",
    "\n",
    "                conf = occurenceData[k+1][newItemset] / supportItemset # calculate confidence: support of `newItemset` divided by support of itemset\n",
    "                if conf > confidence:\n",
    "                    print(itemset, ' ==> ', newItem) \n",
    "\n",
    "associationRules(occurenceData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mVMqrTnWo2T"
   },
   "source": [
    "NOTES:\n",
    "\n",
    "construct is quite fast no need for optimization, filter is slow especially for k=2 (and I don't think there can be more optimization because number of baskets is high)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ID2222-Assignment 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
